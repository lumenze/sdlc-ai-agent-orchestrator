# ğŸ› ï¸ Lumenze SDLC AI Agent â€” Local Development Guide

Welcome to the local development setup for the SDLC AI Agent Orchestrator service. This backend service converts feature requirements into structured Jira tickets using LLMs like GPT-4.

---

## ğŸš€ Project Structure (Overview)

```
sdlc-ai-agent-orchestrator/
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ requirements/            # Sample requirement .md files
â”œâ”€â”€ templates/               # Prompt templates
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ api/                 # FastAPI entrypoint and route handlers
â”‚   â”œâ”€â”€ core/                # LLM client, logger
â”‚   â”œâ”€â”€ config/              # Environment & settings
â”‚   â”œâ”€â”€ utils/               # Markdown â†’ CSV helper, cache
â”‚   â””â”€â”€ services/            # (Optional future service layer)
```

---

## ğŸ“¦ Setup Instructions

### 1. ğŸ” Clone Repo & Create Virtual Env

```bash
git clone https://github.com/your-org/sdlc-ai-agent-orchestrator.git
cd sdlc-ai-agent-orchestrator

python3 -m venv .venv
source .venv/bin/activate
```

---

### 2. ğŸ“„ Install Dependencies

```bash
pip install -r requirements.txt
```

If you're using file upload via FastAPI:
```bash
pip install python-multipart
```

---

### 3. ğŸ” Environment Variables

Create a `.env` file in the root of the repo:

```dotenv
OPENAI_API_KEY=sk-xxx-your-key-here
```

---

### 4. â–¶ï¸ Run the API Server (Dev)

```bash
uvicorn src.api.main:app --reload
```

- Open browser: [http://localhost:8000/docs](http://localhost:8000/docs) to access Swagger UI

---

## ğŸ“¤ Using the API (cURL)

Upload a Markdown file and download Jira ticket CSV:

```bash
curl -X POST http://localhost:8000/generate-tickets   -F "requirements_file=@requirements/samples/login_dashboard.md"   --output generated_tickets.csv
```

---

## ğŸ§ª Running Tests

```bash
pytest tests/
```

---

## ğŸ³ Docker (Optional)

### 1. Build & Run API Container

```bash
docker build -t sdlc-api .
docker run -p 8000:8000 sdlc-api
```

Or use `docker-compose`:

```bash
docker-compose up --build
```

---

## ğŸ—ƒï¸ Caching (Local)

Responses are cached in `.cache/` folder to reduce GPT token usage.

- Clear cache: `rm -rf .cache/`

---

## ğŸ” Logs

Logs are output to the console with timestamp and log level using a centralized logger (`src/core/logger.py`).

---

## ğŸ¤ Contributing

1. Fork repo
2. Create feature branch
3. Push PR with context and test coverage

---

## ğŸ“¬ Questions?

Contact: [Sharon Bose](mailto:bose@lumenze.com)  
Project: [Lumenze AI Orchestrator](https://github.com/your-org/sdlc-ai-agent-orchestrator)

---