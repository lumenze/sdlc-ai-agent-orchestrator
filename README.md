# ğŸ§  Lumenze SDLC AI Agent

An intelligent AI Agent as a Service that integrates deeply with the Lumenze software development lifecycle (SDLC). It streamlines the end-to-end developer workflow â€” from transforming requirements into Jira stories, generating scaffolds, providing IDE assistance, and integrating with CI/CD pipelines.

---

## ğŸ“ Project Structure

```bash
sdlc-ai-agent-orchestrator/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ api/              # FastAPI app entrypoint, routes, and request/response models
â”‚   â”‚   â”œâ”€â”€ main.py
â”‚   â”‚   â”œâ”€â”€ routes/       # API endpoints (e.g., /health, /requirements)
â”‚   â”‚   â””â”€â”€ models/       # Pydantic request/response schemas
â”‚   â”œâ”€â”€ core/             # Core AI logic: LLM clients and orchestration logic
â”‚   â”œâ”€â”€ services/         # Business logic: GitHub, Jira integration, story generation
â”‚   â”œâ”€â”€ utils/            # Shared helper functions
â”‚   â””â”€â”€ config/           # Pydantic settings, env loader
â”œâ”€â”€ tests/                # Unit and integration tests
â”œâ”€â”€ .env                  # Environment variables (not committed)
â”œâ”€â”€ requirements.txt      # Python dependencies
â”œâ”€â”€ .gitignore
â””â”€â”€ README.md             # You are here

âš™ï¸ Key Features
	â€¢	ğŸ§  Centralized LLM Agent (Claude, GPT, Gemini â€“ API routed)
	â€¢	ğŸ“¦ Converts requirement templates into actionable Jira stories
	â€¢	ğŸ› ï¸ Scaffolds technical tasks and code suggestions
	â€¢	ğŸ§© Plugin-ready architecture for GitHub, IDEs, CI/CD
	â€¢	ğŸ” API key/token control (centralized; no client-side exposure)
	â€¢	ğŸ“Š Scalable microservice-ready design

ğŸš§ Phase 1: MVP Milestones

Task
Status
ğŸ”¹ Setup FastAPI skeletonâœ… Done
ğŸ”¹ Health + LLM API routesâ¬œ TODO
ğŸ”¹ Env + Pydantic configâ¬œ TODO
ğŸ”¹ Claude / GPT LLM routingâ¬œ TODO
ğŸ”¹ Req â†’ Jira story pipelineâ¬œ TODO
ğŸ”¹ CI/CD starter integrationâ¬œ TODO

ğŸ§‘â€ğŸ’» Local Development
# 1. Create a virtual environment
python3 -m venv venv && source venv/bin/activate

# 2. Install dependencies
pip install -r requirements.txt

# 3. Run the FastAPI server
uvicorn src.api.main:app --reload

ğŸ”’ Notes on Secrets and Keys

All LLM API keys and config settings are loaded via .env.
A sample .env.template is provided for onboarding new devs.

---

## ğŸ§  Architecture Overview

The Lumenze SDLC AI Agent is a **modular, scalable AI agent-as-a-service** that supports:

- Intelligent SDLC automation (PM â†’ Stories â†’ Code)
- Developer-side LLM copiloting (bug fixing, code suggestions)
- IDE plugins + secure backend inference
- Centralized usage tracking, token control, and API security

### ğŸ”Œ Key Components

| Module             | Description |
|--------------------|-------------|
| `src/api/`         | FastAPI app entry, routes, request/response contracts |
| `src/core/llm_client.py` | Unified LLM abstraction over Claude, GPT, Gemini |
| `src/core/orchestrator.py` | Coordinates requirement â†’ story â†’ code generation |
| `src/services/`    | Integrations (Jira, GitHub, IDE, etc.) |
| `src/utils/`       | Shared helpers |
| `src/config/`      | Settings via `pydantic-settings` |

---

## ğŸ§ª Design Principles

1. **Modularity:** Each component (API, LLM, GitHub, Jira) is decoupled and pluggable.
2. **Testability:** All services and agents support async unit testing via `pytest-asyncio`.
3. **Security:** LLM API keys are backend-only (not exposed to client/IDE).
4. **Scalability:** Designed to run on EC2 with future Docker/K8s deploy targets.
5. **Observability:** Future support for logging, tracebacks, token/cost tracking.
6. **Developer UX:** IDE plugins and FastAPI frontend for PM/Eng interaction.

---

## âš™ï¸ Technology Stack

| Layer           | Tooling                  |
|-----------------|--------------------------|
| Language        | Python 3.12              |
| API             | FastAPI + Uvicorn        |
| LLMs            | OpenAI GPT-4, Claude, Gemini |
| HTTP            | `httpx`, `python-multipart` |
| Config          | `pydantic`, `.env` via `pydantic-settings` |
| Testing         | `pytest`, `pytest-asyncio` |
| CI/CD (future)  | GitHub Actions, auto-deploy pipelines |
| IDE Integration | VS Code, JetBrains (via plugin) |
| Hosting Target  | EC2 (with future ECS/K8s options) |

---

## ğŸ” AI Inference Routing

LLM calls are routed based on:
- Model strengths (e.g., Claude for planning, GPT-4 for reasoning, Gemini for coding)
- Task type (e.g., story generation, code fix)
- Cost vs token limits

Future enhancements:
- Centralized router service with token-based usage quota
- Feedback loop for model tuning via structured prompts

---

## ğŸ“Š SDLC Use Case Flow
PM/BA submits requirements via UI
â†“
Agent generates tech stories, epics
â†“
Jira integration to create tickets
â†“
Engineer picks up ticket from UI/IDE
â†“
Agent generates scaffold, helper code
â†“
Developer edits, tests, commits to GitHub
â†“
CI/CD pipeline runs validations

---

## ğŸ” Future Enhancements
- CI/CD GitHub Action support (auto-format, lint, test)
- Cost control by LLM + model-level quotas
- Intelligent feedback collection for agent learning
- Slack + SharePoint integrations for collaboration
- Prompt templating + prompt chaining
